{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T20:04:21.106347300Z",
     "start_time": "2025-01-07T20:03:54.888453Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "\n",
    "# Załadowanie danych\n",
    "file_path = \"lemmatized_mbsa.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierwsze 5 wierszy danych:\n",
      "         Date                                               text Sentiment  \\\n",
      "0  2019-05-27  cardano digitize currencies eos roi atampt bit...  Positive   \n",
      "1  2019-05-27  another test tweet that wasnt caught in the st...  Positive   \n",
      "2  2019-05-27  current crypto prices btc usd eth usd ltc usd ...  Positive   \n",
      "3  2019-05-27  spiv nosar baz bitcoin is an asset amp not a c...  Positive   \n",
      "4  2019-05-27  we have been building on the real bitcoin sv w...  Positive   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  cardano digitize currency eos roi atampt bitco...  \n",
      "1                test tweet not catch stream bitcoin  \n",
      "2  current crypto price btc usd eth usd ltc usd b...  \n",
      "3          spiv nosar baz bitcoin asset amp currency  \n",
      "4             build real bitcoin sv build broken btc  \n",
      "\n",
      "Kolumny w zbiorze danych: Index(['Date', 'text', 'Sentiment', 'lemmatized_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Podgląd danych\n",
    "print(\"Pierwsze 5 wierszy danych:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sprawdzanie kolumn w zbiorze danych\n",
    "print(\"\\nKolumny w zbiorze danych:\", data.columns)\n",
    "\n",
    "# Upewnienie się, że wszystkie teksty są w formacie string\n",
    "data['lemmatized_text'] = data['lemmatized_text'].astype(str)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T20:05:22.801935Z",
     "start_time": "2025-01-07T20:05:21.972907500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wektoryzacja zakończona. Dane zapisano do plików 'tfidf_matrix.npz', 'tfidf_vectorizer.pkl' i 'labels.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Wektoryzacja TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "# Zapis wektoryzowanych danych do pliku (w formacie .npz dla oszczędności miejsca)\n",
    "scipy.sparse.save_npz(\"tfidf_matrix.npz\", tfidf_matrix)\n",
    "\n",
    "# Zapis wektoryzera do pliku, aby móc odtworzyć te same cechy później\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Zapis etykiet\n",
    "data['Sentiment'].to_csv(\"labels.csv\", index=False)\n",
    "\n",
    "print(\"Wektoryzacja zakończona. Dane zapisano do plików 'tfidf_matrix.npz', 'tfidf_vectorizer.pkl' i 'labels.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T20:11:39.663866200Z",
     "start_time": "2025-01-07T20:05:41.566420100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
